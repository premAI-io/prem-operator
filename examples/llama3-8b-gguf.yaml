apiVersion: premlabs.io/v1alpha1
kind: AIModelMap
metadata:
  name: llama-3-8b-gguf
spec:
  localai:
      - variant: base
        uri: "huggingface://bartowski/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q8_0.gguf"
        engineConfigFile: |
          name: gpt-4-1106-preview
          mmap: true
          parameters:
            model: huggingface://bartowski/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q8_0.gguf
          context_size: 8196
          template:
            chat_message: |
              <|start_header_id|>{{.RoleName}}<|end_header_id|>

              {{ .Content }}<|eot_id|>
            chat: |
              {{ .Input }}
              <|start_header_id|>assistant<|end_header_id|>
---
apiVersion: premlabs.io/v1alpha1
kind: AIDeployment 
metadata:
  name: llama-3-8b-gguf
  namespace: default
spec:
  engine:
    name: "localai" 
    options:
      imageTag: sha-220958a-cublas-cuda12-ffmpeg-core
  endpoint:
    - port: 8080 
      domain: "llama-3-8b-gguf.127.0.0.1.nip.io"
  models:
    - modelMapRef:
        name: llama-3-8b-gguf
        variant: base
  deployment:
    accelerator:
      interface: "CUDA"
      minVersion:
        major: 7
    resources:
      requests:
        cpu: 4
        memory: 8Gi
      limits:
        cpu: 32
        memory: "16Gi"
  env:
    - name: "DEBUG"
      value: "true"
--- 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-3-tui
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-3-tui
  template:
    metadata:
      labels:
        app: llama-3-tui
    spec:
      containers:
      - name: llama-3-tui
        image: premai/elia
        env:
        - name: OPENAI_API_BASE
          value: "http://llama-3-8b-gguf:8080"
        ports:
        - containerPort: 3000
        stdin: true
        tty: true
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-3-cli-conf
data:
  extra-openai-models.yaml: |
    - model_id: gpt-4-1106-preview
      model_name: gpt-4-1106-preview
      api_base: http://llama-3-8b-gguf:8080
  default_model.txt: |
    gpt-4-1106-preview
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-3-cli
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-3-cli
  template:
    metadata:
      labels:
        app: llama-3-cli
    spec:
      initContainers:
      - name: cp-conf
        image: premai/llm-cli
        command: ["sh", "-c", "cp -v /conf/* /root/.config/io.datasette.llm/"]
        volumeMounts:
        - name: conf
          mountPath: /conf/
        - name: data
          mountPath: /root/.config/io.datasette.llm/
      containers:
      - name: llama-3-cli
        image: premai/llm-cli
        stdin: true
        tty: true
        volumeMounts:
        - name: data
          mountPath: /root/.config/io.datasette.llm/
      volumes:
      - name: conf
        configMap:
          name: llama-3-cli-conf
      - name: data
        emptyDir: {}
